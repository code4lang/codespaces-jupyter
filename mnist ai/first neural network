import numpy as np


from keras.datasets import mnist
import sigmoid
(X_train, y_train), (X_test, y_test) = mnist.load_data()
Train=(X_train, y_train)
Test=(X_test, y_test)

X_train = X_train.reshape(60000, 784)
X_test = X_test.reshape(10000, 784)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')
import matplotlib.pyplot as plt
plt.figure(figsize=(7.195, 3.841), dpi=100)
for i in range(200):
  plt.subplot(10,20,i+1)
  plt.imshow(X_train[i,:].reshape([28,28]), cmap='gray')
  plt.axis('off')
plt.show()
'''
print('X_train: ' + str(train_X.shape))
print('Y_train: ' + str(train_y.shape))
print('X_test:  '  + str(test_X.shape))
print('Y_test:  '  + str(test_y.shape))
'''
m, n = [i.shape for i in (X_train, y_train)]
print(m,n)
network_depth=int(input("How many layers ?"))

def init_params(width_l1,width_l2):
    W1 = np.random.rand(width_l2, width_l1)
    b1 = np.random.rand(width_l2, 1)   
    return [W1, b1]
'''
print([len(i[0]) for i in (init_params(28**2,10))])
for i in range (100):
     print(np.random.rand(10, 10))
     print(np.random.randn(20,10) * 0.01) '''

def activation(Z):
    return sigmoid.sigmoid(Z)
def deriv_activation(Z):
    return sigmoid.deriv_sigmoid(Z)

#/workspaces/codespaces-jupyter/activation functions/softmax.py

def forward_prop(W1, b1, X):
    X1 = activation(W1.dot(X) + b1)
    return X1
def test(Y):
    one_hot_Y = np.zeros((Y.size, Y.max() + 1))
    one_hot_Y[np.arange(Y.size), Y] = 1
    one_hot_Y = one_hot_Y.T
    return one_hot_Y
def backward_prop(alpha, W1, b1, Y):
    one_hot_Y = test(Y)
    
    W2 = W1 - alpha*(Y*deriv_activation(W1*Y+b1)*(Y-one_hot_Y))
    b2 = b1 - alpha*(deriv_activation(W1*Y+b1)*(Y-one_hot_Y))
    
    return [W2, b2]


def get_predictions(A2):
    return np.argmax(A2, 0)

def get_accuracy(predictions, Y):
    print(predictions, Y)
    return np.sum(predictions == Y) / Y.size


def training(X,alpha, iterations):
    dim=[28**2,70,10,10]
    params=[]
    perceptrons=[X]
    for i in range(iterations):
        
        for n in range(network_depth):
            par=init_params(dim[n],dim[n+1])
            neuron=forward_prop(W1=par[0], b1=par[1], X=perceptrons[n])
            if i==0:
                params.append(init_params(dim[n],dim[n+1]))
                perceptrons.append(forward_prop(W1=params[n][0], b1=params[n][1], X=perceptrons[n]))         
            else:               
                params[n]=par
                perceptrons[n]=neuron
            print("layer {0}".format(n))
        
        for n in range(network_depth-1,-1,-1):
            params[n] = backward_prop(alpha, W1=params[n][0], b1=params[n][1], Y=perceptrons[n])
       
        if i % 10 == 0:
            print("Iteration: ", i)
            predictions = get_predictions(A2)
            print(get_accuracy(predictions, Y))

    return None
#training(Train[0],alpha=0.01, iterations=500)